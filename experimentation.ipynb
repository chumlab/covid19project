{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation\n",
    "\n",
    "Loading the datasets and cleaning. The following datasets are expected:\n",
    "* `locations_clean_user_location.tsv`: The original provided list of raw locations with corresponding number of occurances\n",
    "* In `/data`:\n",
    "  * `cities1000.tsv`, cities with > 1000 pop. (GeoNames):\n",
    "    * https://download.geonames.org/export/dump/cities1000.zip\n",
    "    * Unzipped and renamed to `.tsv`\n",
    "  * `countryInfo.tsv`, countries (GeoNames):\n",
    "    * https://download.geonames.org/export/dump/countryInfo.txt\n",
    "    * Unzipped and renamed to `.tsv`\n",
    "  * `admin1CodesASCII.txt`, states and provinces (admin1) (GeoNames)\n",
    "    * https://download.geonames.org/export/dump/admin1CodesASCII.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "data_dir = \"data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_user_location</th>\n",
       "      <th>tweet_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>4994911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States</td>\n",
       "      <td>190257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>India</td>\n",
       "      <td>97652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tweet_user_location  tweet_id\n",
       "0                None   4994911\n",
       "1       United States    190257\n",
       "2               India     97652"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tweets user locations list\n",
    "# Loading using pandas' read_csv (tab-deleted) to set 'tweet_id' dtype to int\n",
    "\n",
    "tweets_user_locations = os.path.join(current_dir, \"locations_clean_user_location.tsv\")\n",
    "df = pd.read_csv(tweets_user_locations, sep='\\t', dtype={'tweet_id': int})\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pndurette/.pyenv/versions/3.8.3/envs/chumblab/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3071: DtypeWarning: Columns (13) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geonameid</th>\n",
       "      <th>name</th>\n",
       "      <th>asciiname</th>\n",
       "      <th>altnames</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>featclass</th>\n",
       "      <th>featcode</th>\n",
       "      <th>country</th>\n",
       "      <th>cc2</th>\n",
       "      <th>admin1</th>\n",
       "      <th>admin2</th>\n",
       "      <th>admin3</th>\n",
       "      <th>admin4</th>\n",
       "      <th>population</th>\n",
       "      <th>elevation</th>\n",
       "      <th>gtopo30</th>\n",
       "      <th>timezone</th>\n",
       "      <th>moddate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3039154</td>\n",
       "      <td>El Tarter</td>\n",
       "      <td>El Tarter</td>\n",
       "      <td>Ehl Tarter,Эл Тартер</td>\n",
       "      <td>42.57952</td>\n",
       "      <td>1.65362</td>\n",
       "      <td>P</td>\n",
       "      <td>PPL</td>\n",
       "      <td>AD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1052</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1721</td>\n",
       "      <td>Europe/Andorra</td>\n",
       "      <td>2012-11-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3039163</td>\n",
       "      <td>Sant Julià de Lòria</td>\n",
       "      <td>Sant Julia de Loria</td>\n",
       "      <td>San Julia,San Julià,Sant Julia de Loria,Sant J...</td>\n",
       "      <td>42.46372</td>\n",
       "      <td>1.49129</td>\n",
       "      <td>P</td>\n",
       "      <td>PPLA</td>\n",
       "      <td>AD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>921</td>\n",
       "      <td>Europe/Andorra</td>\n",
       "      <td>2013-11-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3039604</td>\n",
       "      <td>Pas de la Casa</td>\n",
       "      <td>Pas de la Casa</td>\n",
       "      <td>Pas de la Kasa,Пас де ла Каса</td>\n",
       "      <td>42.54277</td>\n",
       "      <td>1.73361</td>\n",
       "      <td>P</td>\n",
       "      <td>PPL</td>\n",
       "      <td>AD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2363</td>\n",
       "      <td>2050.0</td>\n",
       "      <td>2106</td>\n",
       "      <td>Europe/Andorra</td>\n",
       "      <td>2008-06-09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   geonameid                 name            asciiname  \\\n",
       "0    3039154            El Tarter            El Tarter   \n",
       "1    3039163  Sant Julià de Lòria  Sant Julia de Loria   \n",
       "2    3039604       Pas de la Casa       Pas de la Casa   \n",
       "\n",
       "                                            altnames  latitude  longitude  \\\n",
       "0                               Ehl Tarter,Эл Тартер  42.57952    1.65362   \n",
       "1  San Julia,San Julià,Sant Julia de Loria,Sant J...  42.46372    1.49129   \n",
       "2                      Pas de la Kasa,Пас де ла Каса  42.54277    1.73361   \n",
       "\n",
       "  featclass featcode country  cc2 admin1 admin2 admin3 admin4  population  \\\n",
       "0         P      PPL      AD  NaN     02    NaN    NaN    NaN        1052   \n",
       "1         P     PPLA      AD  NaN     06    NaN    NaN    NaN        8022   \n",
       "2         P      PPL      AD  NaN     03    NaN    NaN    NaN        2363   \n",
       "\n",
       "   elevation  gtopo30        timezone     moddate  \n",
       "0        NaN     1721  Europe/Andorra  2012-11-03  \n",
       "1        NaN      921  Europe/Andorra  2013-11-23  \n",
       "2     2050.0     2106  Europe/Andorra  2008-06-09  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GeoNames (Cities with > 1000 inabitants)\n",
    "# https://download.geonames.org/export/dump/cities1000.zip\n",
    "# Loading using geopandas for geometry (usefulness tbd)\n",
    "\n",
    "cities = os.path.join(current_dir, data_dir, \"cities1000.tsv\")\n",
    "# cities_df = gpd.read_file(cities)\n",
    "cities_df = pd.read_csv(cities, sep='\\t',\n",
    "            names=['geonameid', 'name', 'asciiname', 'altnames', 'latitude', 'longitude',\n",
    "                   'featclass', 'featcode', 'country', 'cc2', 'admin1', 'admin2', 'admin3', 'admin4',\n",
    "                   'population', 'elevation', 'gtopo30', 'timezone', 'moddate'])\n",
    "            #dtype={'admin3': str})\n",
    "cities_df.columns = cities_df.columns.str.lower() # lowercase headers\n",
    "cities_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "geopandas.geodataframe.GeoDataFrame"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test cities finding\n",
    "# City\n",
    "city_test = cities_df[(cities_df['name'] == 'London')]\n",
    "city_test\n",
    "\n",
    "# City & admin1\n",
    "city_test = cities_df[(cities_df['name'] == 'London') & \\\n",
    "                      (cities_df['admin1'] == 'ENG')]\n",
    "city_test\n",
    "\n",
    "# City & admin1 & country\n",
    "city_test = cities_df[(cities_df['name'] == 'London') & \\\n",
    "                      (cities_df['admin1'] == 'ENG') & \\\n",
    "                      (cities_df['country'] == 'GB')]\n",
    "city_test\n",
    "\n",
    "# City & country\n",
    "city_test = cities_df[(cities_df['name'] == 'London') & \\\n",
    "                      (cities_df['country'] == 'GB')]\n",
    "city_test\n",
    "\n",
    "# City (with the largest population)\n",
    "cities_df[(cities_df['name'] == 'London')].nlargest(1, ['population']) \n",
    "city_test\n",
    "\n",
    "# City with alternative names\n",
    "\n",
    "city_test = cities_df[(cities_df['name'] == 'London')].copy()\n",
    "city_test\n",
    "\n",
    "# city_test.apply(lambda x: x.astype('str').str.split(',')).explode('altnames')\n",
    "\n",
    "# city_test.set_index(['altnames']).apply(lambda x: x.str.split(',').explode()).reset_index()\n",
    "# city_test.set_index(['name', 'altnames']).apply(lambda x: x.astype(str).str.split(',').explode()).reset_index()\n",
    "# city_test\n",
    "# city_test.explode(column='altnames', ignore_index=False)\n",
    "# city_test.explode('A')\n",
    "\n",
    "# df = pd.DataFrame({'A': [1, 'foo', 1, 1], 'B': 1})\n",
    "# df\n",
    "# df.explode('A')\n",
    "\n",
    "type(city_test)\n",
    "\n",
    "\n",
    "#cities_df.explode('altnames')\n",
    "\n",
    "# (df.set_index(['order_id', 'order_date'])\n",
    "#    .apply(lambda x: x.str.split(',').explode())\n",
    "#    .reset_index()) \n",
    "\n",
    "\n",
    "# city_test = cities_df[(cities_df['name'] == 'LON') | ('LON' in cities_df['altnames'].str.split(',') )]\n",
    "# city_test = cities_df[('LON' in cities_df['altnames'].str.split(',') )]\n",
    "# city_test\n",
    "\n",
    "# df = df[~df['tweet_user_location'].isin(tweet_user_location_discard)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeoNames (Countries info)\n",
    "# https://download.geonames.org/export/dump/countryInfo.txt\n",
    "# Loading using pandas' read_csv (tab-deleted), ignore lines 1-48\n",
    "\n",
    "countries = os.path.join(current_dir, data_dir, \"countryInfo.tsv\")\n",
    "countries_df = pd.read_csv(countries, sep='\\t', header=49)\n",
    "countries_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeoNames (states and provinces, admin1)\n",
    "# https://download.geonames.org/export/dump/admin1CodesASCII.txt\n",
    "# Loading using pandas' read_csv (tab-deleted),\n",
    "# Column names from https://download.geonames.org/export/dump/readme.txt\n",
    "# 'code' is '<country>.<admin1 for country>'\n",
    "\n",
    "admin1 = os.path.join(current_dir, data_dir, \"admin1CodesASCII.txt\")\n",
    "admin1_df = pd.read_csv(admin1, sep='\\t', names=['code', 'name', 'name ascii', 'geonameid'])\n",
    "admin1_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GeoNames 'admin1' (admin1_df) for Canadian provinces uses a 2-digit code\n",
    "# Use postal abbreviation which people use\n",
    "\n",
    "# CA.01\tAlberta\tAlberta\t5883102\n",
    "# CA.02\tBritish Columbia\tBritish Columbia\t5909050\n",
    "# CA.03\tManitoba\tManitoba\t6065171\n",
    "# CA.04\tNew Brunswick\tNew Brunswick\t6087430\n",
    "# CA.13\tNorthwest Territories\tNorthwest Territories\t6091069\n",
    "# CA.07\tNova Scotia\tNova Scotia\t6091530\n",
    "# CA.14\tNunavut\tNunavut\t6091732\n",
    "# CA.08\tOntario\tOntario\t6093943\n",
    "# CA.09\tPrince Edward Island\tPrince Edward Island\t6113358\n",
    "# CA.10\tQuebec\tQuebec\t6115047\n",
    "# CA.11\tSaskatchewan\tSaskatchewan\t6141242\n",
    "# CA.12\tYukon\tYukon\t6185811\n",
    "# CA.05\tNewfoundland and Labrador\tNewfoundland and Labrador\t6354959\n",
    "\n",
    "province_abbr = {\n",
    "    'CA.01': 'CA.AB', # Alberta\n",
    "    'CA.02': 'CA.BC', # British Columbia\n",
    "    'CA.03': 'CA.MB', # Manitoba\n",
    "    'CA.04': 'CA.NB', # New Brunswick\n",
    "    'CA.05': 'CA.NL', # Newfoundland and Labrador\n",
    "    'CA.07': 'CA.NS', # Nova Scotia\n",
    "    'CA.08': 'CA.ON', # Ontario\n",
    "    'CA.09': 'CA.PE', # Prince Edward Island\n",
    "    'CA.10': 'CA.QC', # Quebec\n",
    "    'CA.11': 'CA.SK', # Saskatchewan\n",
    "    'CA.12': 'CA.YK', # Yukon\n",
    "    'CA.13': 'CA.NT', # Northwest Territories\n",
    "    'CA.14': 'CA.NU'  # Nunavut\n",
    "}\n",
    "\n",
    "new_provinces = admin1_df[admin1_df['code'].str.contains('^CA.')].copy()\n",
    "new_provinces['code'] = new_provinces['code'].map(province_abbr)\n",
    "admin1_df = pd.concat([admin1_df, new_provinces], ignore_index=True)\n",
    "admin1_df[admin1_df['code'].str.contains('^CA.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test when there's more than 1 admin1\n",
    "\n",
    "# admin1_df[admin1_df['code'].str.contains('^US.')]\n",
    "# admin1_df[admin1_df['name'] == 'La Paz']\n",
    "country_code = 'HN'\n",
    "element = 'La Paz'\n",
    "test1 = admin1_df[(admin1_df['name'] == element)].copy()\n",
    "test1.loc[:, 'geonameid'] = 99\n",
    "test11 = test1.head(1)\n",
    "test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add alternative country names (e.g. USA, UK, etc.)\n",
    "# (we can't easily get alternative country names)\n",
    "alternative_country_names = {\n",
    "    6252001: 'USA',    # United States\n",
    "    2510769: 'España', # [Kingdom of] Spain\n",
    "    2635167: 'UK',     # United Kingdom\n",
    "    1861060: '日本',    # Japan\n",
    "    298795: 'Türkiye', # Turkey\n",
    "    3469034: 'Brasil', # Brazil\n",
    "    3175395: 'Italia', # Italy\n",
    "    1694008: 'Republic of the Philippines', # Philipines\n",
    "    2921044: 'Deutschland' # Germany\n",
    "}\n",
    "\n",
    "new_countries = pd.DataFrame([], columns=countries_df.columns)\n",
    "for geo, alt_name in alternative_country_names.items():\n",
    "    alt_country = countries_df[countries_df['geonameid'] == geo].copy()\n",
    "    alt_country['Country'] = alt_name\n",
    "    countries_df = pd.concat([countries_df, alt_country], ignore_index=True)\n",
    "\n",
    "# TODO: City alternartives\n",
    "# And others like abbreviations (e.g. CDMX)\n",
    "# Add to list of city alternatives\n",
    "# #3527646: 'CDMX',   # Mexico City "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discard specific 'tweet_user_location' strings\n",
    "LOCATION_DISCARD = ['None', '\\\\N', 'Global', 'Earth',\n",
    "                    'Planet Earth', 'Worldwide', 'Everywhere',\n",
    "                    'Internet', 'En todas partes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the percentage of tweets that have a 'geonameid'\n",
    "# Skipping the ones we know aren't valid (discards)\n",
    "def print_geonameid_completeness(df):\n",
    "    all_tweets = df[~df['tweet_user_location'].isin(LOCATION_DISCARD)]['tweet_id'].sum()\n",
    "    geonameid_tweets = df[df.geonameid.notnull()]['tweet_id'].sum()\n",
    "    print(f'{geonameid_tweets/all_tweets*100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of 'tweet_user_location' so we leave the original intact\n",
    "df['tweet_user_location_copy'] = df['tweet_user_location']\n",
    "\n",
    "# Discard specific 'tweet_user_location' strings\n",
    "# tweet_user_location_discard = ['None', '\\\\N']\n",
    "# df = df[~df['tweet_user_location'].isin(tweet_user_location_discard)]\n",
    "\n",
    "# Discard locations that don't exist more than 2 times\n",
    "df = df[df['tweet_id'] > 2]\n",
    "\n",
    "# Filter out emojis and other symbols\n",
    "# * https://stackoverflow.com/a/49986645\n",
    "# * https://www.ling.upenn.edu/courses/Spring_2003/ling538/UnicodeRanges.html (Unicode symbol ranges)\n",
    "import re\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emojis: emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # emojis: symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # emojis: transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # emojis: flags (iOS)\n",
    "        u\"\\U00002700-\\U000027BF\"  # 'Dingbats' http://www.unicode.org/charts/PDF/U2700.pdf\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "df['tweet_user_location_copy'] = df['tweet_user_location_copy'].map(lambda x: deEmojify(x))\n",
    "\n",
    "# Truncate leading and trailing spaces\n",
    "df['tweet_user_location_copy'] = df['tweet_user_location_copy'].map(lambda x: x.strip())\n",
    "\n",
    "# Truncate trailing \",\" and \".\" characters\n",
    "df['tweet_user_location_copy'] = df['tweet_user_location_copy'].map(lambda x: x.rstrip(','))\n",
    "df['tweet_user_location_copy'] = df['tweet_user_location_copy'].map(lambda x: x.rstrip('.'))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some locations are verbatim the name of a country, e.g.:\n",
    "df[df['tweet_user_location_copy'] == 'Canada']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df[df['tweet_user_location'].isin(simple_countries_df['Country'])]\n",
    "# # simple_countries_df = countries_df[['#ISO','Country', 'geonameid']]#.set_index('Country')\n",
    "\n",
    "# # Merge in country info (with goenameid) when there's an exact country match\n",
    "\n",
    "# # Keep the columns of countries_df we need.\n",
    "# simple_countries_df = countries_df[['#ISO','Country', 'geonameid']]\n",
    "# #df = pd.merge(df, simple_countries_df, how='left', left_on='tweet_user_location_copy', right_on='Country')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print_geonameid_completeness(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple_cities_df = cities_df[['geonameid', 'name', 'asciiname', 'altnames']]\n",
    "# df = pd.merge(df, simple_cities_df, how='left', left_on='tweet_user_location', right_on='name')\n",
    "#df_copy = df[df['geonameid'].isnull()]\n",
    "#pd.merge(df_copy, simple_cities_df, how='left', left_on='tweet_user_location_copy', right_on='name')\n",
    "\n",
    "# NB: this can't work b/c cities name (unlike countries) aren't unique, e.g. there's a lot of \"London\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['tweet_user_location_copy'].str.count(',') > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = \"Toronto, Ontario, Canada, World\"\n",
    "# test = \"Toronto, Canada\"\n",
    "# test.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def split_fixed_parts(num_parts, location):\n",
    "#     parts = location.split(',')\n",
    "#     if num_parts > len(parts):\n",
    "#         for i in range(num_parts - len(parts)):\n",
    "#             parts.insert(0, None)\n",
    "#     else:\n",
    "#         for i in range(len(parts) - num_parts):\n",
    "#             parts.pop(0)\n",
    "#     return parts\n",
    "\n",
    "# def parts_dict(num_parts, location):\n",
    "#     parts = split_fixed_parts(num_parts, location)\n",
    "#     return {f'el-{k}':parts[k] for k in range(num_parts)}\n",
    "\n",
    "def split_parts(location):\n",
    "    return location.split(',')\n",
    "\n",
    "# print(split_fixed_parts(3, 'Toronto'))\n",
    "print(split_parts('Toronto'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini test dataset\n",
    "# df[['column_new_1', 'column_new_2', 'column_new_3']] = pd.DataFrame([[np.nan, 'dogs', 3]], index=df.index)\n",
    "test_df = df[df['tweet_user_location_copy'].str.count(',') == 0].head(200)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_parts = 3\n",
    "\n",
    "# https://stackoverflow.com/a/16242202\n",
    "#test_df.tweet_user_location_copy.apply(lambda s: pd.Series(parts_dict(num_parts, s)))\n",
    "#test_df = pd.concat([test_df, test_df.tweet_user_location_copy.apply(lambda s: pd.Series(parts_dict(num_parts, s)))], axis=1)\n",
    "\n",
    "test_df['elements'] = test_df['tweet_user_location_copy'].map(lambda location: location.split(','))\n",
    "# test_df['elements'] = test_df['elements'].map(lambda x: [i.strip() for i in x if i is not None])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Case incensitive mask/match\n",
    "# e.g. df2 = df1['company_name'].str.contains(\"apple\", na=False, case=False)\n",
    "\n",
    "def get_country(countries_df, element):\n",
    "    # Filter 'Country' field with 'element'\n",
    "    country = countries_df[countries_df['Country'] == element]\n",
    "    \n",
    "    # No results\n",
    "    if len(country) == 0:\n",
    "        return None\n",
    "    \n",
    "    # There can be only one result\n",
    "    return country\n",
    "\n",
    "\n",
    "def get_admin1(admin1_df, element, country_code=None):\n",
    "    if country_code is None:\n",
    "        # admin1 matching name (or ascii name) w/o country\n",
    "        admin1 = admin1_df[(admin1_df['name'] == element) | \\\n",
    "                           (admin1_df['name ascii'] == element)]\n",
    "    \n",
    "        if len(admin1) == 0:\n",
    "            # No results\n",
    "            return None\n",
    "    \n",
    "        if len(admin1) > 1:\n",
    "            # #ERROR:99\n",
    "            # This error happens when, w/o a country, there is\n",
    "            # more than 1 admin1 by that name/ascii name.\n",
    "            # There is not enough data to infer which one.\n",
    "            # e.g. \"La Paz\" district (Bolivia, Honduras, El Savador)\n",
    "            admin1 = admin1_df[(admin1_df['name'] == element)].copy()\n",
    "            admin1.loc[:, 'geonameid'] = 99\n",
    "            return admin1.head(1)\n",
    "            \n",
    "    \n",
    "    else:\n",
    "        # admin1 matching name (or ascii name) and country\n",
    "        admin1 = admin1_df[(admin1_df['code'].str.contains(f'^{country_code}.')) & \\\n",
    "                          ((admin1_df['name'] == element) | \\\n",
    "                           (admin1_df['name ascii'] == element))]\n",
    "        \n",
    "        if len(admin1) == 0:\n",
    "            # No results\n",
    "            # TODO: Look for an abbreviation else return None\n",
    "            return None\n",
    "    \n",
    "    return admin1\n",
    "\n",
    "\n",
    "def get_city(cities_df, element, admin1_code=None, country_code=None):\n",
    "    if admin1_code is None and country_code is None:\n",
    "        cities = cities_df[(cities_df['name'] == element)]\n",
    "    \n",
    "    elif admin1_code is None:\n",
    "        cities = cities_df[(cities_df['name'] == element) & \\\n",
    "                         (cities_df['country'] == country_code)]\n",
    "    \n",
    "    elif country_code is None:\n",
    "        cities = cities_df[(cities_df['name'] == element) & \\\n",
    "                           (cities_df['admin1'] == admin1_code)]\n",
    "    \n",
    "    else:\n",
    "        cities = cities_df[(cities_df['name'] == element) & \\\n",
    "                           (cities_df['admin1'] == admin1_code) & \\\n",
    "                           (cities_df['country'] == country_code)]\n",
    "    \n",
    "    if len(cities) == 0:\n",
    "        # No results\n",
    "        return None\n",
    "    \n",
    "    else:\n",
    "        # More than one result,\n",
    "        # take the city with the largest population.\n",
    "        return cities.nlargest(1, ['population']) \n",
    "\n",
    "\"\"\"\n",
    "Cases:\n",
    "\n",
    "                  country\n",
    "      state/prov, country\n",
    "            city, country\n",
    "city, state/prov, country\n",
    "\n",
    "city, state/prov, country\n",
    "city, state/prov\n",
    "city, country\n",
    "city\n",
    "\n",
    "neighboorhood, city, country\n",
    "neighboorhood, city\n",
    "\n",
    "state/prov, country\n",
    "state/prov\n",
    "\"\"\"\n",
    "\n",
    "def infer_geonameid(elements):\n",
    "    # Datasets\n",
    "    # * countries_df\n",
    "    # * admin1_df\n",
    "    # * cities_df\n",
    "\n",
    "    # print(elements)\n",
    "    \n",
    "    # Don't try to infer if element should be ignored\n",
    "    if elements[0] in LOCATION_DISCARD:\n",
    "        pass\n",
    "    \n",
    "    # One item\n",
    "    # TODO: Invert? Check city first, then state, then country?\n",
    "    # e.g. New York is always the city, not the state.\n",
    "    elif len(elements) == 1:\n",
    "        country = get_country(countries_df, elements[0])\n",
    "        \n",
    "        # \"<country>\" as-is\n",
    "        if country is not None:\n",
    "            return str(country['geonameid'].item())\n",
    "    \n",
    "        admin1 = get_admin1(admin1_df, elements[0])\n",
    "    \n",
    "        # \"<state/province>\" as-is\n",
    "        if admin1 is not None:\n",
    "            return str(admin1['geonameid'].item())\n",
    "\n",
    "        city = get_city(cities_df, elements[0])\n",
    "        \n",
    "        # \"<city>\" as-is\n",
    "        if city is not None:\n",
    "            return str(city['geonameid'].item())\n",
    "\n",
    "    \n",
    "    # Two items\n",
    "    elif len(elements) == 2:\n",
    "        \n",
    "        # if element[1] is country:\n",
    "            # if element[0] is <state/province> within <country>:\n",
    "                # return <state/province>\n",
    "                \n",
    "            # if element[0] is <city> within <country>:\n",
    "                # return <city>\n",
    "                \n",
    "            # return country\n",
    "        \n",
    "        # if element[1] is <state/province>:\n",
    "            # if element[0] is <city> within <country>:\n",
    "                # return <city>\n",
    "                \n",
    "            # return <state/province>\n",
    "            \n",
    "        # if element[1] is <city>:\n",
    "        \n",
    "            # return <city>\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    # Three items\n",
    "    elif len(elements) == 3:\n",
    "        \n",
    "        # if element[2] is country:\n",
    "            # if element[1] is <state/province> within <country>:\n",
    "                # if element[0] if <city> within <state/province>\n",
    "                    # return <city\n",
    "                    \n",
    "                # return <state/province>\n",
    "                \n",
    "            # if element[1] is city within country:\n",
    "                # return <city>\n",
    "                \n",
    "            # return <country>\n",
    "            \n",
    "        # if element[2] is <state/province>:\n",
    "            # if element[1] if <city> within <state/province>\n",
    "                    # return <city>\n",
    "        \n",
    "        pass\n",
    "            \n",
    "    return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 200)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', None)\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "test_df['geonameid'] = np.nan\n",
    "test_df['geonameid'] = test_df['elements'].map(lambda elements: infer_geonameid(elements))\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_geonameid_completeness(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
